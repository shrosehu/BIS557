---
title: "homework-2"
author: "Rose Hu"
date: "10/20/2019"
output: html_document

vignette: >
  %\VignetteIndexEntry{homework-2}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(casl)
```

1.loss function
$$L= (y-x\hat{\beta}_1)(y-x\hat{\beta}_1^T)
= y^Ty-2\hat{\beta}_1x^Ty+(\hat{\beta}_1)^Tx^Tx\hat{\beta}_1 $$
taking derivative of loss function with respect to $\hat{\beta}_1$,   
$$\hat{\beta}_1= (x^Tx)^{-1}xy$$  

$$X'X = \mathbf{X}_{n \times 2} = \left[\begin{array}
{ccc}
n & \sum x_i \\
\sum x_i & \sum x_i^2  \\
\end{array}\right]$$

 

 inverse of $X'X$ is:

$$(X'X)^{-1} = \frac{1}{n \sum x_i^2 - (\sum x_i)^2}\left[\begin{array}
{ccc}
\sum x_i)^2 & -\sum x_i) \\
-\sum x_i) & n  \\
\end{array}\right]$$

$$X'Y = \left[\begin{array}
{ccc}
1 & 1 & ... & 1 \\
x_1 & x_2 & ... & x_n  \\
\end{array}\right] \left[\begin{array}
{ccc}
y_1 \\
y_2 \\
... \\
y_n \\
\end{array}\right]=\left[\begin{array}
{ccc}
\sum y_i \\
\sum x_i y_i  \\
\end{array}\right]$$



$$\beta = \frac{1}{n \sum x_i^2 - (\sum x_i)^2}\left[\begin{array}
{ccc}
\sum x_i^2 & -\sum x_i) \\
-\sum x_i & n  \\
\end{array}\right]\left[\begin{array}
{ccc}
\sum y_i \\
\sum x_i y_i  \\
\end{array}\right]$$





4. Calculate the condition number and mean square error for this data.
```{r}
library(casl)
n <- 1000; p <- 25
beta <- c(1, rep(0, p - 1))
X <- matrix(rnorm(n * p), ncol = p)
svals <- svd(X)$d
max(svals) / min(svals)
N <- 1e4; l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
```
we get that the condition number is 1.328 and mse is 0.1577.   
Then we make changes to x by making x colinear. Then we calculate condition number and mean square error again. 

```{r}
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
svals <- svd(X)$d
max(svals) / min(svals)

N <- 1e4; l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- solve(crossprod(X), crossprod(X, y))
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
```
As the result shows, the condiiton number and error increased tremendously. 
Now we will show using ridge regression can increase numerical statibility and decrease variance. 
```{r}
lambda <- 0.5
svals <- svd(X)$d
(max(svals)+lambda)/(min(svals)+lambda)

N <- 1e4; l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- solve( crossprod(X) + diag(rep(lambda, ncol(X))) ) %*% t(X) %*% y
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)

```
By adding just a very small number $\lambda=0.5$, the introduced bias decreased the condition number and variance.  



5.Assume that $x^Tx =I$, $x_1$, $x_2$,....$x_p$ are independent  
x \subset $R^{nby1}$  
$\frac{1}{2n} ||Y - X \beta||^2_2 + \lambda ||\beta||_1$  ($\beta>0$)

$L(\beta)=\frac{1}{2n}||Y - X \beta||^2_2+ \lambda \beta$  
$\frac{dL}{d\beta}=\frac{1}{n}(-x^T)(Y-x\beta)+\lambda=0$  
$(-x^T)(Y-x\beta)+u\lambda=0$  
$-x^TY+x^Tx\beta+n\lambda=0$  
$x^Tx\beta=x^TY-n\lambda$  
$\beta = (x^Tx)^{-1}[x^T-n\lambda]$  
$=x^TY-n\lambda$

since $|X^t_j Y| \le n \lambda$, then the result is negative above, which means $\beta=0$
