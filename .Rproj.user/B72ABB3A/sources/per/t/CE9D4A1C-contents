---
title: "homework-2"
author: "Rose Hu"
date: "10/20/2019"
output: html_document

vignette: >
  %\VignetteIndexEntry{homework-2}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(casl)
```

1.loss function
$L= (y-x\hat{\beta}_1(y-x\hat{\beta}_1^T$  
$= y^Ty-2\hat{\beta}_1x^Ty+(\hat{\beta}_1)^Tx^Tx\hat{\beta}_1$  
taking derivative of loss function with respect to $\hat{\beta}_1$,   
$\hat{\beta}_1= (x^Tx)^{-1}xy$  
$\hat{\beta}_0= y-(x^Tx)^(-1)xyx$  

4. Calculate the condition number and mean square error for this data.
```{r}
library(casl)
n <- 1000; p <- 25
beta <- c(1, rep(0, p - 1))
X <- matrix(rnorm(n * p), ncol = p)
svals <- svd(X)$d
max(svals) / min(svals)
N <- 1e4; l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
```
we get that the condition number is 1.328 and mse is 0.1577.   
Then we make changes to x by making x colinear. Then we calculate condition number and mean square error again. 

```{r}
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
svals <- svd(X)$d
max(svals) / min(svals)

N <- 1e4; l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- solve(crossprod(X), crossprod(X, y))
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
```
As the result shows, the condiiton number and mse increased tremendously. 
Now we will show using ridge regression can increase numerical statibility and decrease variance. 
```{r}
lambda <- 0.5
svals <- svd(X)$d
(max(svals)+lambda)/(min(svals)+lambda)

N <- 1e4; l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- solve(crossprod(X)+lambda, crossprod(X, y))
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)

```
By adding just a very small number $\lambda=0.5$, the introduced bias decreased the condition number and variance.  

5.Assume that $x^Tx =I$, $x_1$, $x_2$,....$x_p$ are independent  
x \subset $R^{nby1}$  
$\frac{1}{2n} ||Y - X \beta||^2_2 + \lambda ||\beta||_1$  ($\beta>0$)

$L(\beta)=\frac{1}{2n}||Y - X \beta||^2_2+ \lambda \beta$  
$\frac{dL}{d\beta}=\frac{1}{n}(-x^T)(Y-x\beta)+\lambda=0$  
$(-x^T)(Y-x\beta)+u\lambda=0$  
$-x^TY+x^Tx\beta+n\lambda=0$  
$x^Tx\beta=x^TY-n\lambda$  
$\beta = (x^Tx)^{-1}[x^T-n\lambda]$  
$=x^TY-n\lambda$


